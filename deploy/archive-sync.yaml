---
apiVersion: v1
kind: Template
metadata:
  name: archive-sync
objects:
- apiVersion: cloud.redhat.com/v1alpha1
  kind: ClowdApp
  metadata:
    name: archive-sync
  spec:
    envName: ${ENV_NAME}
    objectStore:
      - ${SOURCE_S3_BUCKET}
      - ${TARGET_S3_BUCKET}
    testing:
      iqePlugin: ccx
    dependencies:
      - ingress
    deployments:
      - name: instance
        minReplicas: ${{MIN_REPLICAS}}
        webServices:
          public:
            enabled: false
          private:
            enabled: false
          metrics:
            enabled: true
        podSpec:
          image: ${IMAGE}:${IMAGE_TAG}
          command: ["ccx-messaging"]
          args: ["/data/config.yaml"]
          env:
            - name: CLOWDER_ENABLED
              value: ${CLOWDER_ENABLED}

            # Kafka configuration
            - name: KAFKA_GROUP_ID
              value: ${KAFKA_GROUP_ID}
            - name: KAFKA_PLATFORM_SERVICE
              value: ${KAFKA_PLATFORM_SERVICE}
            - name: KAFKA_CONSUMER_SERVER
              value: ${KAFKA_CONSUMER_SERVER}
            - name: KAFKA_CONSUMER_SECURITY_PROTOCOL
              value: ${KAFKA_CONSUMER_SECURITY_PROTOCOL}
            - name: KAFKA_CONSUMER_SASL_MECHANISM
              value: ${KAFKA_CONSUMER_SASL_MECHANISM}
            - name: KAFKA_INCOMING_TOPIC
              value: ${KAFKA_INCOMING_TOPIC}

            # S3 Configuration: secrets are populated by Clowder
            - name: SOURCE_S3_BUCKET
              value: ${SOURCE_S3_BUCKET}
            - name: TARGET_S3_BUCKET
              value: ${TARGET_S3_BUCKET}

            # TODO: Configure Sentry when the PoC is done
            # - name: SENTRY_DSN
            #   valueFrom:
            #     secretKeyRef:
            #       key: dsn
            #       name: archive-sync-dsn
            #       optional: true
            # - name: SENTRY_ENVIRONMENT
            #   value: ${ENV_NAME}
            - name: ALLOW_UNSAFE_LINKS
              value: ${ALLOW_UNSAFE_LINKS}
            - name: LOGGING_TO_CW_ENABLED
              value: "True"
            - name: CW_STREAM_NAME
              value: ${CW_LOG_STREAM}
            - name: AWS_REGION_NAME
              valueFrom:
                secretKeyRef:
                  name: cloudwatch
                  key: aws_region
                  optional: true
            - name: CW_LOG_GROUP
              valueFrom:
                secretKeyRef:
                  name: cloudwatch
                  key: log_group_name
                  optional: true
            - name: CW_AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: cloudwatch
                  key: aws_access_key_id
                  optional: true
            - name: CW_AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: cloudwatch
                  key: aws_secret_access_key
                  optional: true

            - name: LOGLEVEL_CCX_MESSAGING
              value: ${LOGLEVEL_CCX_MESSAGING}
            - name: LOGLEVEL_INSIGHTS_MESSAGING
              value: ${LOGLEVEL_INSIGHTS_MESSAGING}
            - name: LOGLEVEL_INSIGHTS
              value: ${LOGLEVEL_INSIGHTS}
            - name: LOGLEVEL_KAFKA
              value: ${LOGLEVEL_KAFKA}
            - name: LOGLEVEL_STDOUT
              value: ${LOGLEVEL_STDOUT}
            - name: LOGLEVEL_ROOT
              value: ${LOGLEVEL_ROOT}
          readinessProbe:
            httpGet:
              path: /metrics
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 20
            successThreshold: 1
            periodSeconds: 10
            timeoutSeconds: 10
          livenessProbe:
            httpGet:
              path: /metrics
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 20
            successThreshold: 1
            failureThreshold: 1
            periodSeconds: 10
            timeoutSeconds: 10
          startupProbe:
            httpGet:
              path: /metrics
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 20
            successThreshold: 1
            # Wait for 10x10 seconds
            failureThreshold: 10
            periodSeconds: 10
            timeoutSeconds: 10
          volumeMounts:
            - mountPath: /data
              name: archive-sync-config
          volumes:
            - configMap:
                name: archive-sync-config-map
              name: archive-sync-config
          resources:
            requests:
              cpu: ${CPU_REQUEST}
              memory: ${MEMORY_REQUEST}
            limits:
              cpu: ${CPU_LIMIT}
              memory: ${MEMORY_LIMIT}

    kafkaTopics:
      - replicas: 16
        partitions: 3
        topicName: ${KAFKA_INCOMING_TOPIC}
      - replicas: 1
        partitions: 1
        topicName: ${KAFKA_DEAD_LETTER_QUEUE_TOPIC}
      - replicas: 3
        partitions: 1
        topicName: ${PAYLOAD_TRACKER_TOPIC}

- kind: ConfigMap
  apiVersion: v1
  metadata:
    labels:
      app: archive-sync
    name: archive-sync-config-map
  data:
    config.yaml: |-
      plugins:
        packages:
        - ccx_messaging
      service:
        extract_timeout: ${EXTRACT_TIMEOUT}
        extract_tmp_dir: ${EXTRACT_TMP_DIR}
        format: insights.formats._json.JsonFormat
        target_components: []
        consumer:
          name: ccx_messaging.consumers.kafka_consumer.KafkaConsumer
          kwargs:
            incoming_topic: ${KAFKA_INCOMING_TOPIC}
            platform_service: ${KAFKA_PLATFORM_SERVICE}
            group.id: ${KAFKA_GROUP_ID}
            bootstrap.servers: ${KAFKA_CONSUMER_SERVER}
            processing_timeout_s: 0
            max.poll.interval.ms: 600000
            heartbeat.interval.ms: 10000
            session.timeout.ms: 20000
            dead_letter_queue_topic: ${KAFKA_DEAD_LETTER_QUEUE_TOPIC}

        downloader:
          name: ccx_messaging.downloaders.s3_downloader.S3Downloader
          kwargs:
            access_key: ${SOURCE_S3_ACCESS_KEY}
            secret_key: ${SOURCE_S3_SECRET_KEY}
            endpoint_url: ${SOURCE_S3_URL}
            bucket: ${SOURCE_S3_BUCKET}

        engine:
          name: ccx_messaging.engines.s3_upload_engine.S3UploadEngine
          kwargs:
            dest_bucket: ${TARGET_S3_BUCKET}
            access_key: ${TARGET_S3_ACCESS_KEY}
            secret_key: ${TARGET_S3_SECRET_KEY}
            endpoint: ${TARGET_S3_URL}
            archives_path_prefix: archives/compressed
        publisher:
          name: insights_messaging.publishers.Publisher
        watchers:
          - name: ccx_messaging.watchers.stats_watcher.StatsWatcher
            kwargs:
              prometheus_port: 8000

        logging:
          version: 1
          disable_existing_loggers: false
          handlers:
            default:
              level: ${LOGLEVEL_STDOUT:INFO}
              class: logging.StreamHandler
              stream: ext://sys.stdout
              formatter: default

          formatters:
            default:
              format: '%(asctime)s %(name)s %(levelname)-8s %(message)s'
              datefmt: '%Y-%m-%d %H:%M:%S'

          root:
            level: ${LOGLEVEL_ROOT:WARNING}
            handlers:
              - default

          loggers:
            ccx_messaging:
              level: ${LOGLEVEL_CCX_MESSAGING:DEBUG}

            insights_messaging:
              level: ${LOGLEVEL_INSIGHTS_MESSAGING:DEBUG}

            insights:
              level: ${LOGLEVEL_INSIGHTS:WARNING}

            kafka:
              level: ${LOGLEVEL_KAFKA:INFO}

- kind: Service
  apiVersion: v1
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "8000"
      prometheus.io/scheme: http
      prometheus.io/scrape: "true"
    name: archive-sync-prometheus-exporter
    labels:
      app: archive-sync
  spec:
    ports:
      - name: archive-sync-port-metrics
        port: 8000
        protocol: TCP
        targetPort: 8000
    selector:
      app: archive-sync

parameters:
- description: Image name
  name: IMAGE
  value: quay.io/cloudservices/ccx-messaging
- description: Image tag
  name: IMAGE_TAG
  required: true
- description: Determines Clowder deployment
  name: CLOWDER_ENABLED
  value: "true"
- description: ClowdEnv Name
  name: ENV_NAME
  required: true
- description: Minimum number of pods to use when autoscaling is enabled
  name: MIN_REPLICAS
  value: '1'
- description: Minimum number of pods to use when autoscaling is enabled
  name: MAX_REPLICAS
  value: '1'

- name: KAFKA_INCOMING_TOPIC
  value: platform.upload.announce
  required: true
- name: KAFKA_SYNCED_TOPIC
  value: eph-archive-synced
  required: true
- name: KAFKA_DEAD_LETTER_QUEUE_TOPIC
  value: ccx.archive.sync.dead.letter.queue
  required: true
- name: PAYLOAD_TRACKER_TOPIC
  value: platform.payload-status
  required: true
- name: KAFKA_GROUP_ID
  value: archive_sync_app
  required: true
- name: KAFKA_CONSUMER_SERVER
  description: External data pipeline Kafka
  value: mq-kafka:29092
  required: true
- description: Service name for filtering received Kafka message from announce topic
  name: KAFKA_PLATFORM_SERVICE
  required: true
  value: openshift
- name: KAFKA_CONSUMER_SECURITY_PROTOCOL
  value: SASL_SSL
  required: true
- name: KAFKA_CONSUMER_SASL_MECHANISM
  value: SCRAM-SHA-512
  required: true

- name: SOURCE_S3_BUCKET
  value: source-bucket
  required: true
- name: TARGET_S3_BUCKET
  value: target-bucket
  required: true

- name: CLOUDWATCH_DEBUG
  value: "false"
  required: true
- name: CW_LOG_STREAM
  value: "archive-sync"
- name: ALLOW_UNSAFE_LINKS
  value: ""

- name: CPU_LIMIT
  value: 200m
- name: MEMORY_LIMIT
  value: 512Mi
- name: CPU_REQUEST
  value: 100m
- name: MEMORY_REQUEST
  value: 256Mi

- name: LOGLEVEL_CCX_MESSAGING
  value: DEBUG
- name: LOGLEVEL_INSIGHTS_MESSAGING
  value: DEBUG
- name: LOGLEVEL_INSIGHTS
  value: WARNING
- name: LOGLEVEL_KAFKA
  value: INFO
- name: LOGLEVEL_STDOUT
  value: INFO
- name: LOGLEVEL_ROOT
  value: WARNING
